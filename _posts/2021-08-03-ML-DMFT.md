---
title: "Machine learning  assisted prediction of metal-insulator transition in dynamical mean field theory"
toc: true
toc_sticky: true
date: 2021-08-03 20:07:00
categories: 
  - Physics
tags: 
  - 2021-spring
  - 2021-summer
use_math: true
published : true
---

This work is a collaborative work, thus my contribution to this research is
- Conducting DMFT-NRG and obtain data from it.
- Train neural network model with DMFT-NRG hybridization function, calculate accuracy
- Transformation of weight matrix to ensure that the input layer receives the Matsubara frequency domain data.
- Test our neural network model with the hybridization function on the Matsubara frequency domain. 
- Analyze the structure of neural network.

This is the remaining research that my contribution is smaller.
- ...and find the quantity $S_2$, which is similar to order parameter.
- Train the linear perceptron model with $S_2$, confirm that it is the order parameter.


## Introduction of DMFT
Dynamic mean-field theory is the method to describe the properties of the strongly correlated materials.
DMFT maps many-body lattice problem into many-body local problem which is the impurity problem.
Using the Anderson impurity model, it is now solvable with the self-consistency equation.
Various solvers are such as NRG(numerical renormalization group), ED(exact diagonalization), or QMC(quantum Monte-Carlo).

Especially, DMFT-NRG is the method that can calculate the exact spectral function on the real frequency. For the Bethe lattice, the spectral function is equivalent to the imaginary part of the hybridization function, and therefore, we will predict the phase transition of the DMFT on the Hubbard model by training a simple neural network model with the hybridization function. Furthermore, we will test this model on the Matsubara frequency.

### DMFT-NRG calculation and its data
Dynamical Mean Field Theory with Anderson impurity model solver as 'numerical renormalization group' method. One DMFT loop consists of the following steps.
1. Make DMFT approximation $\Sigma\simeq\Sigma_{imp}$
2. Calculate local Green function
3. Compute hybridization function, and new impurity Green function. 
4. Calculate new $\Sigma$ using self consistency

In Bethe lattice, self consistency: $\Delta = D^2 G/4$. 

Moreover, DMFT-NRG has characteristics of logarithmic discretization, which is employed during the calculation to ensure its continuity near zero frequency. (Vast majority of the data is centered near zero frequency.)

<center><img src="/assets/images/DMFTNN/w_dist.png" width="60%" height="60%"></center>

Implementing broyden method with hybridization function have increased the speed of calculation. 
$$
\Delta_{out} = dmftstep(\Delta_{in}))\\
0 = F(\Delta_{new}) = dmftstep(\Delta_{in})-\Delta_{in}
$$

Finally, calculated phase transition points $U_{c1}$ and $U_{c2}$ are

<center>
  
| $1/T$    | 100   | 1000  | 10000 |
| -------- | ----- | ----- | ----- |
| $U_{c1}$ | 2.200 | 2.225 | 2.250 |
| $U_{c2}$ | 2.50  | 2.69  | 2.9   |
  
</center>
  
<center><img src="/assets/images/DMFTNN/betaall.png" width="75%" height="75%"></center>


## Training the neural network
The scheme of training and testing the neural network model is illustrated below. The input layer of the model receives the imaginary part of the hybridization function, and gives the probability of being in each phase as an output. (The output vector consists of $P_{metal}$ and $P_{insul}$.) 
1. Training step: the input layer receives the imaginary part of the hybridization function on the real frequency domain. The model is tested to see the accuracy of prediction, on real frequency domain.
2. Transformation: $\mathbf{W}^{(1)}\to \mathbf{W}^\prime^{(1)}$. Details in [here](#transformation-of-the-weight-matrix).
3. Testing step: the input layer receives the imaginary part of the hybrdization function on the Matsubara frequency domain. We use data obtained from DMFT-NRG(use spectral function to extract Green function on the Matsubara frequency domain) and DMFT-ED for testing model.

<center><img src="/assets/images/DMFTNN/flow.png" width="80%" height="80%"></center>

$$
\mathbf{y}=\mathbf{W}^{(1)}(\Delta(\omega_n+i\eta))^{\mathbf{T}} + \mathbf{b}^{(1)}, \\
\mathbf{z}=\mathbf{W}^{(2)}\sigma(\mathbf{y})+ \mathbf{b}^{(2)}, \\
\mathbf{P}=\sigma(\mathbf{z})
$$
where $\sigma(x) = 1/(1+\exp(-x))$ is the sigmoid function. The training set consists of the hybridization function from DMFT-NRG, for given on-site interactions $U_I^{(tr)}={u\mid 3.0\leq u\leq 5.0}$ and $U_M^{(tr)}={u\mid 0.5\leq u\leq 2.0}$, which excludes the intermediate region of phase coexistence. Furthermore, we choose three different kinds of models, with various complexity; Neural network with one hidden layer of hidden node $N_h=100$, $N_h=10$, and logistic regression with no hidden layer.

### Transformation of the weight matrix
This step ensures that the testing-step model receives data on the Matsubara frequency domain. The formula below is based on the Bethe lattice condition.

$$
\text{Im}\Delta(i\omega_n) = \frac{1}{4}\text{Im}G(i\omega_n) = \text{Im}\left(\frac{1}{4}\int_{-\infty}^\infty\frac{A(\omega')\ud \omega'}{i\omega_n-\omega'}\right)
= -\text{Im}\left(\frac{1}{\pi}\int_{-\infty}^\infty\frac{\text{Im}\Delta(\omega')\ud \omega'}{i\omega_n-\omega'}\right)
\\
\mathbf{W}_j^{(1)}(i\omega_n) = -\text{Im}\left(\frac{1}{\pi}\int_{-\infty}^\infty\frac{\mathbf{W}_j^{(1)}(\omega')\ud \omega'}{i\omega_n-\omega'}\right)
$$

### Training and prediction on the real frequency domain
After training, we have simply tested whether the model works well on the real frequency domain. Test data from DMFT-NRG. We have used 4 kinds of test data in this section. 

As shown below, test outputs are quite awesome!

<center>
  
|Test data, DMFT-NRG | Bethe | Cubic |
| -------- | ----- | ----- |
| $T$ | $0.01, \;0.001, \;0.0001$ | $0.0001$ |
  
</center>

#### Neural network, $N_h=100$

<center><img src="/assets/images/DMFTNN/NN_output_u0.001_100.png" width="100%" height="100%"></center>
<center><img src="/assets/images/DMFTNN/NN_outputcubic_u0.001_100.png" width="33%" height="33%"></center>

#### Neural network, $N_h=10$

<center><img src="/assets/images/DMFTNN/NN_output_u0.001_10.png" width="100%" height="100%"></center>
<center><img src="/assets/images/DMFTNN/NN_outputcubic_u0.001_10.png" width="33%" height="33%"></center>

#### Logistic regression

<center><img src="/assets/images/DMFTNN/LogisticRegression_output_u0.01.png" width="100%" height="100%"></center>
<center><img src="/assets/images/DMFTNN/LogisticRegression_outputcubic_u0.01.png" width="33%" height="33%"></center>


### Training and prediction on the Matsubara frequency domain

<center><img src="/assets/images/DMFTNN/result_Matsubara.png" width="85%" height="85%"></center>


## Reference
1. <https://en.wikipedia.org/wiki/Dynamical_mean-field_theory>
2. R. Bulla, T. A. Costi, D. Vollhardt, <https://link.aps.org/doi/10.1103/RevModPhys.80.395>
